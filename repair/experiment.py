# This script is a minimal replication of the main experiment from https://arxiv.org/abs/2211.08403.
# It runs in 30 seconds on an A100.

import copy
from tqdm import tqdm
import matplotlib.pyplot as plt

import torch
from torch import nn
import airbench


ACTIVATION_TYPE = nn.GELU


# Converts the network architecture generated by airbench94 or airbench95 into nn.Sequential form.
def convert_sequential(net):
    return nn.Sequential(
        net[0],
        net[1],
        net[2].conv1, net[2].pool,  net[2].norm1, net[2].activ,
        net[2].conv2, net[2].norm2, net[2].activ,
        net[3].conv1, net[3].pool,  net[3].norm1, net[3].activ,
        net[3].conv2, net[3].norm2, net[3].activ,
        net[4].conv1, net[4].pool,  net[4].norm1, net[4].activ,
        net[4].conv2, net[4].norm2, net[4].activ,
        net[5],
        net[6],
        net[7],
        net[8],
    )


# Given two networks net0, net1 which each output a feature map of shape NxCxWxH, reshapes
# both outputs to (N*W*H)xC and computes the CxC correlation matrix between the two.
def get_corr_matrix(net0, net1, loader):
    n = len(loader)
    with torch.no_grad():
        net0.eval()
        net1.eval()
        for i, (inputs, _) in enumerate(loader):
            
            out0 = net0(inputs).double()
            out0 = out0.permute(0, 2, 3, 1).reshape(-1, out0.shape[1])
            out1 = net1(inputs).double()
            out1 = out1.permute(0, 2, 3, 1).reshape(-1, out1.shape[1])

            mean0_b = out0.mean(dim=0)
            mean1_b = out1.mean(dim=0)
            sqmean0_b = out0.square().mean(dim=0)
            sqmean1_b = out1.square().mean(dim=0)
            outer_b = (out0.T @ out1) / out0.shape[0]
            if i == 0:
                mean0 = torch.zeros_like(mean0_b)
                mean1 = torch.zeros_like(mean1_b)
                sqmean0 = torch.zeros_like(sqmean0_b)
                sqmean1 = torch.zeros_like(sqmean1_b)
                outer = torch.zeros_like(outer_b)
            mean0 += mean0_b / n
            mean1 += mean1_b / n
            sqmean0 += sqmean0_b / n
            sqmean1 += sqmean1_b / n
            outer += outer_b / n

    cov = outer - torch.outer(mean0, mean1)
    std0 = (sqmean0 - mean0**2).sqrt()
    std1 = (sqmean1 - mean1**2).sqrt()
    corr = cov / (torch.outer(std0, std1) + 1e-4)
    return corr

import scipy.optimize
import numpy as np
def get_perm_map(corr_mtx):
    corr_mtx_a = corr_mtx.cpu().numpy()
    row_ind, col_ind = scipy.optimize.linear_sum_assignment(corr_mtx_a, maximize=True)
    assert (row_ind == np.arange(len(corr_mtx_a))).all()
    perm_map = torch.tensor(col_ind).long()
    return perm_map

def permute_output(perm_map, layer):
    keys = ['weight', 'bias', 'running_mean', 'running_var']
    for key in keys:
        w = getattr(layer, key, None)
        if w is not None:
            w.data = w[perm_map]

def permute_input(perm_map, layer):
    w = layer.weight
    w.data = w[:, perm_map]


# Correct BatchNorm statistics
def reset_bn(model, loader, num_batches=-1):
    for m in model.modules():
        if type(m) == nn.BatchNorm2d:
            m.momentum = None # use simple average
            m.reset_running_stats()
    model.train()
    with torch.no_grad():
        for i, (images, _) in enumerate(loader):
            if num_batches != -1 and i > num_batches:
                break
            output = model(images.cuda())

def mix_weights(net0, net1, alpha=0.5):
    net = copy.deepcopy(net0)
    sd0 = net0.state_dict()
    sd1 = net1.state_dict()
    sd_alpha = {k: (1 - alpha) * sd0[k].cuda() + alpha * sd1[k].cuda()
                for k in sd0.keys()}
    net.load_state_dict(sd_alpha)
    return net


if __name__ == '__main__':

    print('\nStep 1: Train two networks on CIFAR-10 (with identical hyperparameters).\n')
    my_net0 = airbench.train94(run=0)
    my_net1 = airbench.train94(run=1)

    train_loader = airbench.CifarLoader('/tmp/cifar10', train=True, batch_size=1024, aug=dict(flip=True, translate=2))
    test_loader = airbench.CifarLoader('/tmp/cifar10', train=False)

    net0 = copy.deepcopy(convert_sequential(my_net0))
    net1 = copy.deepcopy(convert_sequential(my_net1))
    net1_aligned = copy.deepcopy(net1)

    print('\n\nStep 2: Align the neurons of the second network to match the first network.\n')
    print('net0 accuracy: %.4f' % airbench.evaluate(net0, test_loader))
    print('net1 accuracy: %.4f' % airbench.evaluate(net1, test_loader))

    n = len(net1)
    # Iterate through the network layers, looking for convolutions to align.
    for conv_idx in tqdm(range(n)):
        
        if isinstance(net1[conv_idx], nn.Conv2d):
            
            # Find the first activation after the conv layer.
            act_idx = -1
            for j in range(conv_idx+1, n):
                if isinstance(net1[j], ACTIVATION_TYPE):
                    act_idx = j
                    break
            assert not act_idx == -1, 'Failed to find the activation of convolution at index %d' % conv_idx

            # Find the BatchNorm layer, if it exists.
            bn_idx = -1
            for j in range(conv_idx+1, act_idx):
                assert not isinstance(net1[j], nn.Conv2d)
                if isinstance(net1[j], nn.BatchNorm2d):
                    bn_idx = j
                    break
            
            # Find the next convolution or linear layer.
            next_layer_idx = -1
            for j in range(conv_idx+1, n):
                if isinstance(net1[j], nn.Conv2d) or isinstance(net1[j], nn.Linear):
                    next_layer_idx = j
                    break
            assert not next_layer_idx == -1, 'Failed to find the next layer after convolution at index %d' % conv_idx
            assert next_layer_idx > act_idx, 'Found the next layer before the activation for the current layer. The code would need to be modified to support this.'
            
            # Compute a permutation aligning the neurons of the two layers (https://arxiv.org/abs/1511.07543).
            corr_mtx = get_corr_matrix(net0[:act_idx+1], net1[:act_idx+1], train_loader)
            perm_map = get_perm_map(corr_mtx)
            
            # Permute the neurons of the second network to bring them into alignment with the neurons of the first.
            permute_output(perm_map, net1_aligned[conv_idx])
            if bn_idx != -1:
                permute_output(perm_map, net1_aligned[bn_idx])
            permute_input(perm_map, net1_aligned[next_layer_idx])
            
    # Alignment does not change the functional behavior of the network.
    print('net1 accuracy after alignment: %.4f' % airbench.evaluate(net1_aligned, test_loader))


    print('\n\nStep 3: Interpolate between the two networks.\n')

    accs1 = []
    accs2 = []
    accs3 = []
    accs4 = []
    alpha_range = torch.arange(0, 1.0001, 0.02).tolist()
    print('Trying %d different interpolation values...' % len(alpha_range))
    for alpha in tqdm(alpha_range):
        
        net = mix_weights(net0, net1, alpha)
        acc = airbench.evaluate(net, test_loader, tta_level=0)
        accs1.append(acc)
        
        reset_bn(net, train_loader, num_batches=10)
        acc = airbench.evaluate(net, test_loader, tta_level=0)
        accs2.append(acc)
        
        net = mix_weights(net0, net1_aligned, alpha)
        acc = airbench.evaluate(net, test_loader, tta_level=0)
        accs3.append(acc)
        
        reset_bn(net, train_loader, num_batches=10)
        acc = airbench.evaluate(net, test_loader, tta_level=0)
        accs4.append(acc)


    i = alpha_range.index(0.5)
    print('\nAccuracy values for midpoint network (i.e., alpha = 0.5):\n')
    print('Naive interpolation:\t\t\t%.4f' % accs1[i])
    print('Naive interpolation + reset BN:\t\t%.4f' % accs2[i])
    print('Aligned interpolation:\t\t\t%.4f' % accs3[i])
    print('Aligned interpolation + reset BN:\t%.4f' % accs4[i])

    plt.xlabel('Interpolation alpha')
    plt.ylabel('Test-set accuracy')
    plt.title('CIFAR-10 network interpolation')
    plt.plot(alpha_range, accs1, label='Naive interpolation')
    plt.plot(alpha_range, accs2, label='Naive interpolation + reset bn')
    plt.plot(alpha_range, accs3, label='Aligned interpolation')
    plt.plot(alpha_range, accs4, label='Aligned interpolation + reset bn')
    plt.legend()
    plt.savefig('figure.png', dpi=100, bbox_inches='tight')
    print('Saved figure.')

